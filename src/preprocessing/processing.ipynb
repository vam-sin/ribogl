{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from tqdm.auto import tqdm\n",
    "import pickle as pkl\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id to codon and codon to id\n",
    "id_to_codon = {idx:''.join(el) for idx, el in enumerate(itertools.product(['A', 'T', 'C', 'G'], repeat=3))}\n",
    "codon_to_id = {v:k for k,v in id_to_codon.items()}\n",
    "\n",
    "def make_dataframe(\n",
    "    ribo_fname: str, data_path: str, df_trans_to_seq, count_norm: str = \"mean\"\n",
    "):\n",
    "    ribo_fpath = os.path.join(data_path, ribo_fname)\n",
    "\n",
    "    # Import dataset with ribosome data\n",
    "    df_ribo = pd.read_csv(\n",
    "        ribo_fpath,\n",
    "        sep=\" \",\n",
    "        on_bad_lines=\"warn\",\n",
    "        dtype=dict(gene=\"category\", transcript=\"category\"),\n",
    "    ).rename(columns={\"count\": \"counts\"})\n",
    "\n",
    "    # Define count normalization function\n",
    "    if count_norm == \"max\":\n",
    "        f_norm = lambda x: x / x.max()\n",
    "    elif count_norm == \"mean\":\n",
    "        f_norm = lambda x: x / x.mean()\n",
    "    elif count_norm == \"sum\":\n",
    "        f_norm = lambda x: x / x.sum()\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "    # Create final dataframe\n",
    "    final_df = (\n",
    "        df_ribo.merge(df_trans_to_seq).assign(fname=ribo_fname)\n",
    "        # Filter spurious positions at the end of the sequence\n",
    "        .query(\"position_A_site <= n_codons * 3\")\n",
    "        # Compute normalized counts\n",
    "        .assign(\n",
    "            norm_counts=lambda df: df.groupby(\"gene\", observed=True).counts.transform(\n",
    "                f_norm\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def make_all_dataframes(\n",
    "    data_dirpath: str,\n",
    "    fa_fpath: str,\n",
    "    max_n_codons: int = 2000,\n",
    "    count_norm: str = \"mean\",\n",
    "):\n",
    "    # Import FASTA\n",
    "    data = []\n",
    "    with open(fa_fpath, mode=\"r\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            data.append([record.id, str(record.seq)])\n",
    "\n",
    "    # Create transcripts to sequences mapping\n",
    "\n",
    "    df_trans_to_seq = pd.DataFrame(data, columns=[\"transcript\", \"sequence\"])\n",
    "\n",
    "    # Removes those sequences that have Ns\n",
    "    sequence_has_n = df_trans_to_seq.sequence.str.contains(\"N\", regex=False)\n",
    "    df_trans_to_seq = df_trans_to_seq.loc[~sequence_has_n]\n",
    "\n",
    "    # Number of codons in sequence\n",
    "    df_trans_to_seq = df_trans_to_seq.assign(\n",
    "        n_codons=lambda df: df.sequence.str.len() // 3\n",
    "    )\n",
    "\n",
    "    # Compute and merge dataframes\n",
    "    dfs = [\n",
    "        make_dataframe(\n",
    "            f,\n",
    "            df_trans_to_seq=df_trans_to_seq.drop(\"sequence\", axis=1),\n",
    "            data_path=data_dirpath,\n",
    "            count_norm=count_norm,\n",
    "        )\n",
    "        for f in tqdm(os.listdir(data_dirpath))\n",
    "        if not f.startswith(\"ensembl\")\n",
    "    ]\n",
    "    dfs = pd.concat(dfs)\n",
    "    for col in [\"transcript\", \"gene\", \"fname\"]:\n",
    "        dfs[col] = dfs[col].astype(\"category\")\n",
    "\n",
    "    dfs = dfs.groupby([\"transcript\", \"position_A_site\"], observed=True)\n",
    "\n",
    "    # Average replicates\n",
    "    dfs = dfs.agg(dict(norm_counts=\"mean\", gene=\"first\")).reset_index()\n",
    "    \n",
    "    dfs = dfs.assign(codon_idx=lambda df: df.position_A_site // 3)\n",
    "    dfs = dfs.groupby(\"transcript\", observed=True)\n",
    "    dfs = dfs.agg(\n",
    "        {\n",
    "            \"norm_counts\": lambda x: x.tolist(),\n",
    "            \"codon_idx\": lambda x: x.tolist(),\n",
    "            \"gene\": \"first\",\n",
    "        }\n",
    "    ).reset_index()\n",
    "    dfs = dfs.merge(df_trans_to_seq)\n",
    "\n",
    "    dfs = dfs.assign(\n",
    "        n_annot=lambda df: df.norm_counts.transform(lambda x: len(x))\n",
    "        / (df.sequence.str.len() // 3)\n",
    "    )\n",
    "\n",
    "    dfs = dfs.assign(perc_annot=lambda df: df.n_annot / df.n_codons)\n",
    "\n",
    "    # Filter by max sequence lenght\n",
    "    dfs = dfs.query(\"n_codons<@max_n_codons\")\n",
    "\n",
    "    return dfs\n",
    "\n",
    "# def fucntion sequence to codon ids\n",
    "def sequence2codonids(seq):\n",
    "    codon_ids = []\n",
    "    for i in range(0, len(seq), 3):\n",
    "        codon = seq[i:i+3]\n",
    "        if len(codon) == 3:\n",
    "            codon_ids.append(codon_to_id[codon])\n",
    "\n",
    "    return codon_ids\n",
    "\n",
    "def process_merged_df(df):\n",
    "    # remove transcripts with N in sequence\n",
    "    df = df[df['sequence'].str.contains('N') == False]\n",
    "\n",
    "    codon_seqs = []\n",
    "    sequences = list(df['sequence'])\n",
    "    genes = list(df['gene'])\n",
    "    transcripts = list(df['transcript'])\n",
    "    perc_non_zero_annots = []\n",
    "    norm_counts = list(df['norm_counts'])\n",
    "    codon_idx = list(df[\"codon_idx\"])\n",
    "    annot_seqs = []\n",
    "\n",
    "    for i in range(len(sequences)):\n",
    "        seq = sequences[i]\n",
    "        seq = sequence2codonids(seq)\n",
    "        codon_seqs.append(seq)\n",
    "        codon_idx_sample = codon_idx[i]\n",
    "        # convert to list of int\n",
    "        codon_idx_sample = [int(i) for i in codon_idx_sample[1:-1].split(',')]\n",
    "        annot_seq_sample = []\n",
    "        norm_counts_sample = [float(i) for i in norm_counts[i][1:-1].split(',')]\n",
    "        for j in range(len(seq)):\n",
    "            if j in codon_idx_sample:\n",
    "                annot_seq_sample.append(norm_counts_sample[codon_idx_sample.index(j)])\n",
    "            else:\n",
    "                annot_seq_sample.append(0.0)\n",
    "        annot_seqs.append(annot_seq_sample)\n",
    "\n",
    "        # calculate percentage of non-zero annotations\n",
    "        perc_non_zero_annots.append(sum([1 for i in annot_seq_sample if i != 0.0])/len(annot_seq_sample))\n",
    "\n",
    "    final_df = pd.DataFrame(list(zip(genes, transcripts, codon_seqs, annot_seqs, perc_non_zero_annots)), columns = ['gene', 'transcript', 'codon_sequence', 'annotations', 'perc_non_zero_annots'])\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '/net/lts2gdk0/mnt/scratch/lts2/nallapar/rb-prof/data/Jan_2024/Lina/'\n",
    "LIVER_FOLDER = '/net/lts2gdk0/mnt/scratch/lts2/nallapar/rb-prof/data/Jan_2024/Liver/'\n",
    "# merge the dataframes\n",
    "fa_path = f'{DATA_FOLDER}/reference/ensembl.cds.fa'\n",
    "\n",
    "liver_dfs = make_all_dataframes(LIVER_FOLDER, fa_path)\n",
    "liver_dfs.to_csv(f'{DATA_FOLDER}/merged/LIVER.csv')\n",
    "print(\"Merged Liver CTRL\")\n",
    "\n",
    "ctrl_dir_path =f'{DATA_FOLDER}/CTRL/'\n",
    "ctrl_dfs = make_all_dataframes(ctrl_dir_path, fa_path)\n",
    "ctrl_dfs.to_csv(f'{DATA_FOLDER}/merged/CTRL_AA.csv')\n",
    "print(\"Merged CTRL\")\n",
    "\n",
    "ile_dir_path = f'{DATA_FOLDER}/ILE/'\n",
    "ile_dfs = make_all_dataframes(ile_dir_path, fa_path)\n",
    "ile_dfs.to_csv(f'{DATA_FOLDER}/merged/ILE_AA.csv')\n",
    "print(\"Merged ILE\")\n",
    "\n",
    "leu_dir_path = f'{DATA_FOLDER}/LEU/'\n",
    "leu_dfs = make_all_dataframes(leu_dir_path, fa_path)\n",
    "leu_dfs.to_csv(f'{DATA_FOLDER}/merged/LEU_AA.csv')\n",
    "print(\"Merged LEU\")\n",
    "\n",
    "leu_ile_dir_path = f'{DATA_FOLDER}/LEU_ILE/'\n",
    "leu_ile_dfs = make_all_dataframes(leu_ile_dir_path, fa_path)\n",
    "leu_ile_dfs.to_csv(f'{DATA_FOLDER}/merged/LEU-ILE_AA_remBadRep.csv')\n",
    "print(\"Merged LEU-ILE\")\n",
    "\n",
    "leu_ile_val_dir_path = f'{DATA_FOLDER}/LEU_ILE_VAL/'\n",
    "leu_ile_val_dfs = make_all_dataframes(leu_ile_val_dir_path, fa_path)\n",
    "leu_ile_val_dfs.to_csv(f'{DATA_FOLDER}/merged/LEU-ILE-VAL_AA.csv')\n",
    "print(\"Merged LEU-ILE-VAL\")\n",
    "\n",
    "val_dir_path = f'{DATA_FOLDER}/VAL/'\n",
    "val_dfs = make_all_dataframes(val_dir_path, fa_path)\n",
    "val_dfs.to_csv(f'{DATA_FOLDER}/merged/VAL_AA.csv')\n",
    "print(\"Merged VAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the dataframes\n",
    "liver_df = pd.read_csv(f'{DATA_FOLDER}/merged/LIVER.csv')\n",
    "liver_df_proc = process_merged_df(liver_df)\n",
    "liver_df_proc.to_csv(f'{DATA_FOLDER}/processed/LIVER.csv')\n",
    "print(\"Processed LIVER\")\n",
    "\n",
    "ctrl_df = pd.read_csv(f'{DATA_FOLDER}/merged/CTRL_AA.csv')\n",
    "ctrl_df_proc = process_merged_df(ctrl_df)\n",
    "ctrl_df_proc.to_csv(f'{DATA_FOLDER}/processed/CTRL_AA.csv')\n",
    "print(\"Processed CTRL\")\n",
    "\n",
    "ile_df = pd.read_csv(f'{DATA_FOLDER}/merged/ILE_AA.csv')\n",
    "ile_df_proc = process_merged_df(ile_df)\n",
    "ile_df_proc.to_csv(f'{DATA_FOLDER}/processed/ILE_AA.csv')\n",
    "print(\"Processed ILE\")\n",
    "\n",
    "leu_df = pd.read_csv(f'{DATA_FOLDER}/merged/LEU_AA.csv')\n",
    "leu_df_proc = process_merged_df(leu_df)\n",
    "leu_df_proc.to_csv(f'{DATA_FOLDER}/processed/LEU_AA.csv')\n",
    "print(\"Processed LEU\")\n",
    "\n",
    "leu_ile_df = pd.read_csv(f'{DATA_FOLDER}/merged/LEU-ILE_AA_remBadRep.csv')\n",
    "leu_ile_df_proc = process_merged_df(leu_ile_df)\n",
    "leu_ile_df_proc.to_csv(f'{DATA_FOLDER}/processed/LEU-ILE_AA_remBadRep.csv')\n",
    "print(\"Processed LEU-ILE\")\n",
    "\n",
    "leu_ile_val_df = pd.read_csv(f'{DATA_FOLDER}/merged/LEU-ILE-VAL_AA.csv')\n",
    "leu_ile_val_df_proc = process_merged_df(leu_ile_val_df)\n",
    "leu_ile_val_df_proc.to_csv(f'{DATA_FOLDER}/processed/LEU-ILE-VAL_AA.csv')\n",
    "print(\"Processed LEU-ILE-VAL\")\n",
    "\n",
    "val_df = pd.read_csv(f'{DATA_FOLDER}/merged/VAL_AA.csv')\n",
    "val_df_proc = process_merged_df(val_df)\n",
    "val_df_proc.to_csv(f'{DATA_FOLDER}/processed/VAL_AA.csv')\n",
    "print(\"Processed VAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d8cddb8cf669a224cfe7de41be728b42e6d1e4d2fa8033c260d761c14134291"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
